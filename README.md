# Tourist-Guide-RAG

## **RAG-Based Landmark Retrieval and Evaluation System**

**This project implements a Retrieval-Augmented Generation (RAG) system for answering questions about landmarks using multimodal descriptions (textual and BLIP-generated). It includes data preprocessing, retrieval with ColBERT-style reranking, and evaluation of retrieval quality using the answer_relevancy metric. Also we used Qween for judjing.**

## ğŸ“ Overview

The system:

* Cleans and structures multimodal data (Russian descriptions + English BLIP captions).
* Builds retrieval chunks grouped by landmark (WikiData ID).
* Implements a RAG retriever using bert-base-multilingual-cased with:
* FAISS index for fast coarse retrieval (via [CLS] embeddings).
Token-level ColBERT-style MaxSim reranking for precision.
* Evaluates retrieval quality using the answer_relevancy metric on 100 sampled landmarks.

  
##ğŸ› ï¸ Requirements

bash
pip install -q faiss-cpu transformers torch pandas numpy
Note: No external evaluation libraries (e.g., deepeval, ragas) are used â€” all metrics are computed with the projectâ€™s own model. 

## ğŸ“¦ Data Structure
Input data must contain the following columns:
* WikiData: Unique identifier for each landmark.
* Name: Landmark name (e.g., "Nizhny Novgorod Kremlin").
City: City where the landmark is located.
* description: Russian textual description (may contain NaN).
* en_txt: English BLIP-generated caption (may contain NaN).
  
## ğŸ” Retrieval Pipeline

1. Data Cleaning
2. Chunk Creation
3. RAG Retrieval
   
* Stage 1: FAISS index on [CLS] embeddings for fast candidate retrieval (k*5 candidates).
* Stage 2: Rerank candidates using ColBERT-style MaxSim:
Token-level embeddings.
Max-similarity between query and document tokens.

4. Final score = sum of max-sims weighted by query attention mask.

   
## ğŸ“Š Evaluation: answer_relevancy

Method:

* Sample: 100 unique landmarks (or all if <100).
* Query: "Ğ Ğ°ÑÑĞºĞ°Ğ¶Ğ¸ Ğ¿Ñ€Ğ¾ {Name} Ğ² Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğµ {City}".
* Answer: Top-2 retrieved chunks (joined).
* Metric: Cosine similarity between [CLS] embeddings of question and answer, normalized to [0, 1].
* Final Score: Mean similarity across all samples.
* Output
A single scalar value:

query = "Ğ Ğ°ÑÑĞºĞ°Ğ¶Ğ¸ Ğ¿Ñ€Ğ¾ ĞĞ¸Ğ¶ĞµĞ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ğ¹ ĞšÑ€ĞµĞ¼Ğ»ÑŒ Ğ² ĞĞ¾Ğ²Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğµ"
results, _ = rag.retrieve(query, k=7)

answer_relevancy = 0.9629


â–¶ï¸ Usage

* Prepare your data in a DataFrame data with required columns.
* Run the full script â€” it will:
* Clean data and build chunks.
* Initialize the FAISS + ColBERT RAG system.
* Compute and print answer_relevancy.
* Example test query is included:

ğŸ“ Notes

* The model is multilingual (bert-base-multilingual-cased), supporting mixed Russian/English text.
* BLIP captions are in English; descriptions are in Russian â€” the model handles this mix.
* Evaluation uses the same model for retrieval and scoring â€” ensuring consistency.
* Designed to run on CPU (via faiss-cpu) but will auto-use GPU if available.

  
ğŸ“ Optional Extensions (Bonus)
Add faithfulness metric (check if answer is supported by context).
Add context_recall (check if ground truth is covered by retrieved contexts).
Export evaluation dataset for external tools (ragas, trulens)
